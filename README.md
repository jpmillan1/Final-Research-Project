# Final Research Project â€” Part 2: Data Cleaning

This repository contains an end-to-end preprocessing workflow for cleaning, validating, and standardizing traffic datasets for use in transportation modeling. The project implements **Part 2: Data Cleaning** of the Architecture Alphabet framework and produces a unified SQLite database integrating two major data sources:

- **Trajectory data (`trajs`)** â€” segment-level crossing events.
- **Waypoint data (`waypoint`)** â€” high-frequency GPS pings.

The workflow ensures spatial consistency, removes invalid and error-coded records, standardizes timestamps across heterogeneous formats, and prepares the data for arc-based models, path-based analysis, tensor construction, and machine learning tasks.

---

## ğŸš¦ Features

- Automatic CSV â†’ SQLite ingestion with type inference  
- Cleaning of invalid SegmentId records  
- Removal of error-coded trajectories  
- Deduplication of waypoint timestamps  
- Filtering of fuzzed/outlier GPS data  
- Detection of missing GPS intervals using 3-second gap analysis  
- Full timestamp standardization (UTC, ISO, Unix â†’ local time)  
- External validation using FHWA Data Cleaning and Fusion Tool (DCFT)  
- Output: unified, analysis-ready SQLite database

## ğŸ“ Project Structure

``` 
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ csv_to_sqlite_processor.py
â”‚   â”œâ”€â”€ basic_data_cleaner.py
â”‚   â””â”€â”€ time_standardization_processor.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb
â”œâ”€â”€ Output/    # (ignored in .gitignore)
â”‚   â””â”€â”€ database/
â”‚       â””â”€â”€ unified_database.db
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```


## âš™ï¸ Installation

```bash
pip install -r requirements.txt
```

Or install manually:

```bash
pip install pandas chardet pytz sqlite3
```





## â–¶ï¸ Usage

Run each processing stage in order:
python csv_to_sqlite_processor.py
python basic_data_cleaner.py
python time_standardization_processor.py

Or run the pipeline inside the Jupyter notebook.




ğŸ“Š Validation
This project uses the FHWA Data Cleaning and Fusion Tool to validate:

- Spatial integrity

- Timestamp consistency

- Travel time confidence intervals (<0.1% outside federal bounds)

  


ğŸ“š Architecture Alphabet Context

This component supports:

A â€” Arc-based models (clean SegmentIds, consistent link flows)

B â€” Deep learning models (clean time series)

C â€” Path-based architectures (valid OD trajectories)

E â€” Tensor-based analysis (aligned temporal dimensions)

