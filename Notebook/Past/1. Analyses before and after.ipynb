{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc00768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze before and after Basic Data Cleaner\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ================== CONFIGURE HERE ==================\n",
    "# Adjust this path to where your unified_database.db is located\n",
    "DB_PATH = r\"Output\\database\\unified_database.db\"\n",
    "# ====================================================\n",
    "\n",
    "if not os.path.exists(DB_PATH):\n",
    "    raise FileNotFoundError(f\"Database not found at: {DB_PATH}\")\n",
    "\n",
    "# Database connection\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# ---- 1. Inspect available tables ----\n",
    "tables_df = pd.read_sql(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\",\n",
    "    conn\n",
    ")\n",
    "tables = [t[0] for t in tables_df.values]\n",
    "print(\"Tables found in database:\")\n",
    "print(tables_df)\n",
    "\n",
    "# ---- 2. Count rows for key tables ----\n",
    "def get_rowcount(connection, table_name):\n",
    "    try:\n",
    "        df = pd.read_sql(f\"SELECT COUNT(*) AS n FROM {table_name}\", connection)\n",
    "        return int(df[\"n\"].iloc[0])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "row_counts = {}\n",
    "for table in [\"trajs\", \"waypoint\", \"SegmentId_to_link\"]:\n",
    "    if table in tables:\n",
    "        row_counts[table] = get_rowcount(conn, table)\n",
    "    else:\n",
    "        row_counts[table] = None\n",
    "\n",
    "print(\"\\nRow counts:\")\n",
    "for t, n in row_counts.items():\n",
    "    print(f\"  {t}: {n if n is not None else 'DOES NOT EXIST'}\")\n",
    "\n",
    "# ---- 3. Export cleaned tables to CSV ----\n",
    "exported_files = []\n",
    "\n",
    "for table in [\"trajs\", \"waypoint\"]:\n",
    "    if table in tables:\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
    "        out_name = f\"{table}_cleaned.csv\"\n",
    "        df.to_csv(out_name, index=False)\n",
    "        exported_files.append(out_name)\n",
    "        print(f\"\\nTable '{table}' exported to: {out_name}\")\n",
    "    else:\n",
    "        print(f\"\\nTable '{table}' does not exist, CSV was not exported.\")\n",
    "\n",
    "# ---- 4. Basic statistics ----\n",
    "stats = {}\n",
    "\n",
    "# 4a) For trajs: ErrorCodes distribution (if column exists)\n",
    "if \"trajs\" in tables:\n",
    "    df_trajs = pd.read_sql(\"SELECT * FROM trajs\", conn)\n",
    "    cols_trajs = df_trajs.columns.tolist()\n",
    "    stats[\"trajs\"] = {}\n",
    "\n",
    "    if \"ErrorCodes\" in cols_trajs:\n",
    "        error_counts = df_trajs[\"ErrorCodes\"].fillna(\"\").value_counts()\n",
    "        n_total = len(df_trajs)\n",
    "        n_clean = error_counts.get(\"\", 0)\n",
    "        pct_clean = 100 * n_clean / n_total if n_total > 0 else 0\n",
    "\n",
    "        stats[\"trajs\"][\"total_rows\"] = n_total\n",
    "        stats[\"trajs\"][\"rows_without_errorcodes\"] = n_clean\n",
    "        stats[\"trajs\"][\"pct_without_errorcodes\"] = pct_clean\n",
    "        stats[\"trajs\"][\"errorcodes_distribution\"] = error_counts.to_dict()\n",
    "    else:\n",
    "        stats[\"trajs\"][\"note\"] = \"Column 'ErrorCodes' does not exist in trajs.\"\n",
    "\n",
    "# 4b) For waypoint: fuzzed_point distribution (if exists)\n",
    "if \"waypoint\" in tables:\n",
    "    df_wp = pd.read_sql(\"SELECT * FROM waypoint\", conn)\n",
    "    cols_wp = df_wp.columns.tolist()\n",
    "    stats[\"waypoint\"] = {}\n",
    "\n",
    "    if \"fuzzed_point\" in cols_wp:\n",
    "        fuzz_counts = df_wp[\"fuzzed_point\"].fillna(\"\").value_counts()\n",
    "        n_total = len(df_wp)\n",
    "        n_fuzzed = fuzz_counts.get(\"1\", 0)\n",
    "        pct_fuzzed = 100 * n_fuzzed / n_total if n_total > 0 else 0\n",
    "\n",
    "        stats[\"waypoint\"][\"total_rows\"] = n_total\n",
    "        stats[\"waypoint\"][\"fuzzed_rows\"] = n_fuzzed\n",
    "        stats[\"waypoint\"][\"pct_fuzzed\"] = pct_fuzzed\n",
    "        stats[\"waypoint\"][\"fuzzed_distribution\"] = fuzz_counts.to_dict()\n",
    "    else:\n",
    "        stats[\"waypoint\"][\"note\"] = \"Column 'fuzzed_point' does not exist in waypoint.\"\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# ---- 5. Generate Markdown report ----\n",
    "report_lines = []\n",
    "report_lines.append(f\"# Data Cleaning Report\")\n",
    "report_lines.append(\"\")\n",
    "report_lines.append(f\"- Generated on: {datetime.now().isoformat(timespec='seconds')}\")\n",
    "report_lines.append(f\"- Database: `{DB_PATH}`\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "report_lines.append(\"## Detected Tables\")\n",
    "for t in tables:\n",
    "    report_lines.append(f\"- {t}\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "report_lines.append(\"## Row counts per table\")\n",
    "for t, n in row_counts.items():\n",
    "    if n is None:\n",
    "        report_lines.append(f\"- **{t}**: does not exist in database\")\n",
    "    else:\n",
    "        report_lines.append(f\"- **{t}**: {n} rows\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Section for trajs\n",
    "if \"trajs\" in stats:\n",
    "    report_lines.append(\"## Results for table `trajs`\")\n",
    "    st = stats[\"trajs\"]\n",
    "    if \"total_rows\" in st:\n",
    "        report_lines.append(f\"- Total rows: **{st['total_rows']}**\")\n",
    "        report_lines.append(f\"- Rows without `ErrorCodes`: **{st['rows_without_errorcodes']}** \"\n",
    "                            f\"({st['pct_without_errorcodes']:.2f}%)\")\n",
    "        report_lines.append(\"\")\n",
    "        report_lines.append(\"### `ErrorCodes` distribution\")\n",
    "        for k, v in st[\"errorcodes_distribution\"].items():\n",
    "            label = k if k != \"\" else \"(empty / no error)\"\n",
    "            report_lines.append(f\"- `{label}`: {v}\")\n",
    "    else:\n",
    "        report_lines.append(f\"- Note: {st.get('note', 'No statistics available.')}\")\n",
    "    report_lines.append(\"\")\n",
    "\n",
    "# Section for waypoint\n",
    "if \"waypoint\" in stats:\n",
    "    report_lines.append(\"## Results for table `waypoint`\")\n",
    "    st = stats[\"waypoint\"]\n",
    "    if \"total_rows\" in st:\n",
    "        report_lines.append(f\"- Total rows: **{st['total_rows']}**\")\n",
    "        report_lines.append(f\"- Rows with `fuzzed_point = '1'`: **{st['fuzzed_rows']}** \"\n",
    "                            f\"({st['pct_fuzzed']:.2f}%)\")\n",
    "        report_lines.append(\"\")\n",
    "        report_lines.append(\"### `fuzzed_point` distribution\")\n",
    "        for k, v in st[\"fuzzed_distribution\"].items():\n",
    "            label = k if k != \"\" else \"(empty / NULL)\"\n",
    "            report_lines.append(f\"- `{label}`: {v}\")\n",
    "    else:\n",
    "        report_lines.append(f\"- Note: {st.get('note', 'No statistics available.')}\")\n",
    "    report_lines.append(\"\")\n",
    "\n",
    "# CSV export note\n",
    "report_lines.append(\"## Exported CSV files\")\n",
    "if exported_files:\n",
    "    for f in exported_files:\n",
    "        report_lines.append(f\"- `{f}`\")\n",
    "else:\n",
    "    report_lines.append(\"- No CSV files exported because the corresponding tables were not found.\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Save report\n",
    "report_path = \"data_cleaning_report.md\"\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(report_lines))\n",
    "\n",
    "print(f\"\\nReport generated: {report_path}\")\n",
    "print(\"Open it in VS Code and use Ctrl+Shift+V to preview the Markdown document.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Time Standardization before and after\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================\n",
    "# 1. Locate project root\n",
    "# ============================================\n",
    "if \"__file__\" in globals():\n",
    "    # Executed as a .py script\n",
    "    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), \".\"))\n",
    "else:\n",
    "    # Executed from Jupyter / VS Code notebook\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "\n",
    "# Base paths\n",
    "RAW_ROOT = os.path.join(PROJECT_ROOT, \"data_cleaning_fusion_datasets\")\n",
    "DB_PATH = os.path.join(PROJECT_ROOT, \"Output\", \"database\", \"unified_database.db\")\n",
    "\n",
    "if not os.path.exists(DB_PATH):\n",
    "    raise FileNotFoundError(f\"Processed database not found at: {DB_PATH}\")\n",
    "\n",
    "# ============================================\n",
    "# 2. Map BEFORE (CSV) vs AFTER (DB)\n",
    "# ============================================\n",
    "entities = [\n",
    "    {\n",
    "        \"name\": \"Readings\",\n",
    "        \"before_path\": os.path.join(RAW_ROOT, \"tmc_speed\", \"Readings.csv\"),\n",
    "        \"after_table\": \"Readings\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"trajs\",\n",
    "        \"before_path\": os.path.join(RAW_ROOT, \"trip path\", \"trajs.csv\"),\n",
    "        \"after_table\": \"trajs\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"waypoint\",\n",
    "        \"before_path\": os.path.join(RAW_ROOT, \"waypoint\", \"waypoint.csv\"),\n",
    "        \"after_table\": \"waypoint\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# ============================================\n",
    "# 3. Helper functions\n",
    "# ============================================\n",
    "def load_before_csv(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"⚠️ BEFORE CSV not found: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_after_table(conn, table):\n",
    "    try:\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading table {table} from database: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_time_columns(columns):\n",
    "    cols = []\n",
    "    for c in columns:\n",
    "        cl = c.lower()\n",
    "        if \"time\" in cl or \"timestamp\" in cl or \"date\" in cl:\n",
    "            cols.append(c)\n",
    "    return cols\n",
    "\n",
    "# ============================================\n",
    "# 4. Open processed database\n",
    "# ============================================\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# List available tables\n",
    "tables_df = pd.read_sql(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\",\n",
    "    conn\n",
    ")\n",
    "available_tables = set(tables_df[\"name\"].tolist())\n",
    "print(\"Tables in processed database:\", available_tables)\n",
    "\n",
    "# ============================================\n",
    "# 5. Comparison and report generation\n",
    "# ============================================\n",
    "report_lines = []\n",
    "report_lines.append(\"# Time Standardization — Before & After\\n\")\n",
    "report_lines.append(f\"- Generation date: {datetime.now().isoformat(timespec='seconds')}\")\n",
    "report_lines.append(f\"- Processed database (AFTER): `{DB_PATH}`\\n\")\n",
    "report_lines.append(\"In this report, *BEFORE* refers to raw CSVs in `data_cleaning_fusion_datasets`,\")\n",
    "report_lines.append(\"and *AFTER* refers to the resulting tables inside `unified_database.db`.\\n\")\n",
    "\n",
    "exported_files = []\n",
    "\n",
    "for ent in entities:\n",
    "    name = ent[\"name\"]\n",
    "    before_path = ent[\"before_path\"]\n",
    "    after_table = ent[\"after_table\"]\n",
    "\n",
    "    report_lines.append(f\"## Entity: **{name}**\\n\")\n",
    "\n",
    "    # BEFORE (CSV)\n",
    "    df_before = load_before_csv(before_path)\n",
    "    if df_before is None:\n",
    "        report_lines.append(f\"- ❌ Could not read BEFORE CSV at `{before_path}`.\\n\")\n",
    "        report_lines.append(\"---\\n\")\n",
    "        continue\n",
    "\n",
    "    # AFTER (table in DB)\n",
    "    if after_table not in available_tables:\n",
    "        report_lines.append(f\"- ❌ Table `{after_table}` does not exist in the processed database.\\n\")\n",
    "        report_lines.append(\"---\\n\")\n",
    "        continue\n",
    "\n",
    "    df_after = load_after_table(conn, after_table)\n",
    "    if df_after is None:\n",
    "        report_lines.append(f\"- ❌ Could not read table `{after_table}` from the database.\\n\")\n",
    "        report_lines.append(\"---\\n\")\n",
    "        continue\n",
    "\n",
    "    # ---------------- Row counts ----------------\n",
    "    n_before = len(df_before)\n",
    "    n_after = len(df_after)\n",
    "    report_lines.append(\"### Row counts\")\n",
    "    report_lines.append(f\"- BEFORE (CSV): **{n_before}** rows\")\n",
    "    report_lines.append(f\"- AFTER  (DB): **{n_after}** rows\\n\")\n",
    "\n",
    "    # ---------------- Common columns ----------------\n",
    "    common_cols = sorted(list(set(df_before.columns).intersection(df_after.columns)))\n",
    "    report_lines.append(f\"### Common columns ({len(common_cols)})\")\n",
    "    if common_cols:\n",
    "        report_lines.append(\", \".join(common_cols) + \"\\n\")\n",
    "    else:\n",
    "        report_lines.append(\"No common columns between BEFORE and AFTER.\\n\")\n",
    "        report_lines.append(\"---\\n\")\n",
    "        continue\n",
    "\n",
    "    # ---------------- Time columns ----------------\n",
    "    time_cols = detect_time_columns(common_cols)\n",
    "    if time_cols:\n",
    "        report_lines.append(\"### Time column statistics (ranges)\")\n",
    "        for col in time_cols:\n",
    "            try:\n",
    "                bmin, bmax = df_before[col].min(), df_before[col].max()\n",
    "                amin, amax = df_after[col].min(), df_after[col].max()\n",
    "                report_lines.append(f\"- `{col}`:\")\n",
    "                report_lines.append(f\"   - BEFORE: min = {bmin}, max = {bmax}\")\n",
    "                report_lines.append(f\"   - AFTER : min = {amin}, max = {amax}\")\n",
    "            except Exception as e:\n",
    "                report_lines.append(f\"- `{col}`: error computing ranges ({e})\")\n",
    "        report_lines.append(\"\")\n",
    "    else:\n",
    "        report_lines.append(\"### Time columns\")\n",
    "        report_lines.append(\"No time-related columns detected.\\n\")\n",
    "\n",
    "    # ---------------- Nulls ----------------\n",
    "    report_lines.append(\"### Null values (common columns only)\")\n",
    "    for col in common_cols:\n",
    "        nb = df_before[col].isna().sum()\n",
    "        na = df_after[col].isna().sum()\n",
    "        if nb > 0 or na > 0:\n",
    "            report_lines.append(f\"- `{col}`: BEFORE = {nb} nulls, AFTER = {na} nulls\")\n",
    "    report_lines.append(\"\")\n",
    "\n",
    "    # ---------------- Duplicates ----------------\n",
    "    report_lines.append(\"### Duplicate rows (exact matches)\")\n",
    "    dup_before = df_before.duplicated().sum()\n",
    "    dup_after = df_after.duplicated().sum()\n",
    "    report_lines.append(f\"- BEFORE: {dup_before} duplicated rows\")\n",
    "    report_lines.append(f\"- AFTER : {dup_after} duplicated rows\\n\")\n",
    "\n",
    "    # ---------------- Export sample CSVs ----------------\n",
    "    before_out = f\"{name}_before_for_report.csv\"\n",
    "    after_out = f\"{name}_after_for_report.csv\"\n",
    "\n",
    "    df_before.to_csv(before_out, index=False)\n",
    "    df_after.to_csv(after_out, index=False)\n",
    "\n",
    "    exported_files.append(before_out)\n",
    "    exported_files.append(after_out)\n",
    "\n",
    "    report_lines.append(\"### Exported files\")\n",
    "    report_lines.append(f\"- `{before_out}` (raw BEFORE CSV)\")\n",
    "    report_lines.append(f\"- `{after_out}` (AFTER table from DB)\\n\")\n",
    "    report_lines.append(\"---\\n\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# ============================================\n",
    "# 6. Save Markdown report\n",
    "# ============================================\n",
    "REPORT_PATH = os.path.join(PROJECT_ROOT, \"time_standardization_before_after.md\")\n",
    "with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(report_lines))\n",
    "\n",
    "print(\"\\nReport generated:\")\n",
    "print(\" \", REPORT_PATH)\n",
    "print(\"\\nExported CSV files:\")\n",
    "for f in exported_files:\n",
    "    print(\" \", f)\n",
    "\n",
    "print(\"\\nTo preview the report in VS Code:\")\n",
    "print(\" 1) Open 'time_standardization_before_after.md'\")\n",
    "print(\" 2) Press: Ctrl + Shift + V (Open Preview)\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
