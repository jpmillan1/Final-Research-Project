{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07740397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRIDOR SPEED DISTRIBUTION ANALYSIS\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =========================================\n",
    "# 0. PATH CONFIGURATION\n",
    "# =========================================\n",
    "# PROJECT_ROOT = os.getcwd()  # if you run the script from the project folder\n",
    "PROJECT_ROOT = os.getcwd()   # leave it like this if you are running it from the root\n",
    "\n",
    "DB_PATH = os.path.join(PROJECT_ROOT, \"Output\", \"database\", \"unified_database.db\")\n",
    "\n",
    "RAW_WP_PATH = os.path.join(\n",
    "    PROJECT_ROOT, \"data_cleaning_fusion_datasets\", \"waypoint\", \"waypoint.csv\"\n",
    ")\n",
    "RAW_TP_PATH = os.path.join(\n",
    "    PROJECT_ROOT, \"data_cleaning_fusion_datasets\", \"trip path\", \"trajs.csv\"\n",
    ")\n",
    "\n",
    "OUTPUT_TABLE_DIR = os.path.join(PROJECT_ROOT, \"tables_from_tool\")\n",
    "os.makedirs(OUTPUT_TABLE_DIR, exist_ok=True)\n",
    "\n",
    "# Speed column names in the DB (your case)\n",
    "WAYPOINT_SPEED_COL = \"speed_mph\"\n",
    "TRAJS_SPEED_COL = \"CrossingSpeedMph\"\n",
    "\n",
    "# =========================================\n",
    "# 1. Load RAW INPUTS (before cleaning)\n",
    "# =========================================\n",
    "if not os.path.exists(RAW_WP_PATH):\n",
    "    raise FileNotFoundError(f\"Raw waypoint file not found at: {RAW_WP_PATH}\")\n",
    "if not os.path.exists(RAW_TP_PATH):\n",
    "    raise FileNotFoundError(f\"Raw trajs file not found at: {RAW_TP_PATH}\")\n",
    "if not os.path.exists(DB_PATH):\n",
    "    raise FileNotFoundError(f\"Database not found at: {DB_PATH}\")\n",
    "\n",
    "df_wp_raw = pd.read_csv(RAW_WP_PATH, low_memory=False)\n",
    "df_tp_raw = pd.read_csv(RAW_TP_PATH, low_memory=False)\n",
    "\n",
    "wp_input = len(df_wp_raw)\n",
    "tp_input = len(df_tp_raw)\n",
    "\n",
    "print(\"Input records:\")\n",
    "print(\"  Waypoint:\", wp_input)\n",
    "print(\"  Trajs   :\", tp_input)\n",
    "\n",
    "# =========================================\n",
    "# 2. Load FINAL corridor data from unified_database.db\n",
    "# =========================================\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "df_wp = pd.read_sql(\"SELECT * FROM waypoint\", conn)\n",
    "df_tp = pd.read_sql(\"SELECT * FROM trajs\", conn)\n",
    "conn.close()\n",
    "\n",
    "wp_final = len(df_wp)\n",
    "tp_final = len(df_tp)\n",
    "\n",
    "print(\"\\nFinal corridor records (from unified_database.db):\")\n",
    "print(\"  Waypoint:\", wp_final)\n",
    "print(\"  Trajs   :\", tp_final)\n",
    "\n",
    "# =========================================\n",
    "# 3. Compute impacts by cleaning stage\n",
    "# =========================================\n",
    "# For now we DO NOT know the duplicate/outlier/error flags,\n",
    "# so we set them to 0 and assign ALL the difference\n",
    "# to \"Map Matching Filter\".\n",
    "#\n",
    "# If later you find flag columns (e.g., in the DB or in a CSV),\n",
    "# just update these values or compute them directly from the data.\n",
    "\n",
    "# ------- ADJUSTABLE: if you know totals per stage, set them here -------\n",
    "wp_dup = 0            # waypoint records removed as duplicates\n",
    "wp_out = 0            # waypoint records removed as outliers\n",
    "wp_err = 0            # waypoint records removed as errors\n",
    "\n",
    "tp_dup = 0            # trajs records removed as duplicates\n",
    "tp_out = 0            # trajs records removed as outliers\n",
    "tp_err = 0            # trajs records removed as errors\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "wp_removed_total = wp_input - wp_final\n",
    "tp_removed_total = tp_input - tp_final\n",
    "\n",
    "wp_mapfilter = max(wp_removed_total - (wp_dup + wp_out + wp_err), 0)\n",
    "tp_mapfilter = max(tp_removed_total - (tp_dup + tp_out + tp_err), 0)\n",
    "\n",
    "# =========================================\n",
    "# 4. Build \"Cleaning Impact Summary\" table\n",
    "# =========================================\n",
    "def fmt_change(count, base):\n",
    "    \"\"\"\n",
    "    Format like: -310,512 (1.59%)\n",
    "    \"\"\"\n",
    "    if base == 0:\n",
    "        pct = 0.0\n",
    "    else:\n",
    "        pct = 100.0 * count / base\n",
    "    sign = \"-\" if count > 0 else \"\"\n",
    "    return f\"{sign}{count:,} ({pct:.2f}%)\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "rows.append({\n",
    "    \"Cleaning Action\": \"Input Records\",\n",
    "    \"Waypoint Impact\": f\"{wp_input:,}\",\n",
    "    \"Trip Path Impact\": f\"{tp_input:,}\",\n",
    "})\n",
    "rows.append({\n",
    "    \"Cleaning Action\": \"Duplicate Removal\",\n",
    "    \"Waypoint Impact\": fmt_change(wp_dup, wp_input),\n",
    "    \"Trip Path Impact\": fmt_change(tp_dup, tp_input),\n",
    "})\n",
    "rows.append({\n",
    "    \"Cleaning Action\": \"Outlier Removal\",\n",
    "    \"Waypoint Impact\": fmt_change(wp_out, wp_input),\n",
    "    \"Trip Path Impact\": fmt_change(tp_out, tp_input),\n",
    "})\n",
    "rows.append({\n",
    "    \"Cleaning Action\": \"Error Data Removal\",\n",
    "    \"Waypoint Impact\": fmt_change(wp_err, wp_input),\n",
    "    \"Trip Path Impact\": fmt_change(tp_err, tp_input),\n",
    "})\n",
    "rows.append({\n",
    "    \"Cleaning Action\": \"Map Matching Filter\",\n",
    "    \"Waypoint Impact\": fmt_change(wp_mapfilter, wp_input),\n",
    "    \"Trip Path Impact\": fmt_change(tp_mapfilter, tp_input),\n",
    "})\n",
    "rows.append({\n",
    "    \"Cleaning Action\": \"Final Corridor Records\",\n",
    "    \"Waypoint Impact\": f\"{wp_final:,} ({100*wp_final/wp_input:.2f}%)\",\n",
    "    \"Trip Path Impact\": f\"{tp_final:,} ({100*tp_final/tp_input:.2f}%)\",\n",
    "})\n",
    "\n",
    "cleaning_table = pd.DataFrame(rows)\n",
    "\n",
    "print(\"\\n=== CLEANING IMPACT SUMMARY ===\")\n",
    "print(cleaning_table.to_string(index=False))\n",
    "\n",
    "cleaning_csv_path = os.path.join(OUTPUT_TABLE_DIR, \"cleaning_impact_summary.csv\")\n",
    "cleaning_table.to_csv(cleaning_csv_path, index=False)\n",
    "print(f\"\\nCleaning summary table saved to: {cleaning_csv_path}\")\n",
    "\n",
    "# =========================================\n",
    "# 5. \"Corridor Speed Distribution Analysis\"\n",
    "# =========================================\n",
    "def speed_summary(series):\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    return {\n",
    "        \"Count\": int(s.size),\n",
    "        \"Mean (mph)\": s.mean(),\n",
    "        \"Median (mph)\": s.median(),\n",
    "        \"Std Dev (mph)\": s.std(),\n",
    "        \"Min (mph)\": s.min(),\n",
    "        \"Max (mph)\": s.max(),\n",
    "        \"Q25 (mph)\": s.quantile(0.25),\n",
    "        \"Q75 (mph)\": s.quantile(0.75),\n",
    "    }\n",
    "\n",
    "if WAYPOINT_SPEED_COL not in df_wp.columns:\n",
    "    raise KeyError(f\"Column {WAYPOINT_SPEED_COL} not found in waypoint table\")\n",
    "if TRAJS_SPEED_COL not in df_tp.columns:\n",
    "    raise KeyError(f\"Column {TRAJS_SPEED_COL} not found in trajs table\")\n",
    "\n",
    "wp_stats = speed_summary(df_wp[WAYPOINT_SPEED_COL])\n",
    "tp_stats = speed_summary(df_tp[TRAJS_SPEED_COL])\n",
    "\n",
    "speed_table = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"Waypoint\": wp_stats,\n",
    "        \"Trajs\": tp_stats\n",
    "    },\n",
    "    orient=\"index\"\n",
    ").round(2)\n",
    "\n",
    "speed_table = speed_table[\n",
    "    [\"Count\", \"Mean (mph)\", \"Median (mph)\", \"Std Dev (mph)\",\n",
    "     \"Min (mph)\", \"Max (mph)\", \"Q25 (mph)\", \"Q75 (mph)\"]\n",
    "]\n",
    "\n",
    "print(\"\\n=== CORRIDOR SPEED DISTRIBUTION ANALYSIS ===\")\n",
    "print(speed_table.to_string())\n",
    "\n",
    "speed_csv_path = os.path.join(OUTPUT_TABLE_DIR, \"corridor_speed_distribution.csv\")\n",
    "speed_table.to_csv(speed_csv_path)\n",
    "print(f\"\\nSpeed distribution table saved to: {speed_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
