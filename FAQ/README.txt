# â“ Frequently Asked Questions (FAQ)

This FAQ document addresses common questions about the workflow, data, environment, and tools used in this project.  
It is intended to help users reproduce the results and understand how the different components interact.

---

## ğŸ“Œ 1. What is the purpose of this project?
This project implements **Part 2: Data Cleaning** of the Architecture Alphabet framework.  
It uses the FHWA Data Cleaning & Fusion Tool (DCFT) to generate the initial unified dataset and applies additional Python-based cleaning, filtering, and timestamp standardization.

---

## ğŸ“Œ 2. Do I need the FHWA Data Cleaning & Fusion Tool to run this project?
**Yes.**  
The project depends on the outputs produced by the FHWA DCFT application.

You must run the FHWA tool first to generate:

- `unified_database.db`  
- `mapmatching/mapmatching.csv`  
- Raw BEFORE datasets (`data_cleaning_fusion_datasets/`)

Download the tool from:  
https://github.com/usdot-jpo-codehub/data-cleaning-and-fusion-tool

---

## ğŸ“Œ 3. Which raw data files are required?
You must extract **Synthesize Data.zip** from the FHWA repository.

From that ZIP, the two files required for this project are:

- `trajs.csv`  
- `waypoints.csv`  

These are used as INPUT to the FHWA DCFT tool.

---

## ğŸ“Œ 4. What do the Python scripts do?
### `basic_data_cleaner.py`
- Removes invalid SegmentIds  
- Removes error-coded rows  
- Removes fuzzed GPS points  
- Deduplicates waypoint records  
- Detects missing 3-second GPS gaps  

### `time_standardization_processor.py`
- Converts UTC/ISO timestamps into local time  
- Standardizes all timestamp formats  
- Adds `local_time` fields  
- Ensures temporal alignment across all tables  

---

## ğŸ“Œ 5. What database does the project use?
The project uses **SQLite** through:

Output/database/unified_database.db


This file is generated by the FHWA tool and updated by the Python scripts.

Because the file is too large, it is **not included** in the repository.

---

## ğŸ“Œ 6. Are Jupyter Notebooks required?
Not strictly, but **recommended**.

The notebook `analysis.ipynb` is used for:

- Convergence plots  
- Cleaning performance metrics  
- Cross-source speed validation  
- Table inspection  
- Generating figures used in the report  

You may run it with:


jupyter notebook notebooks/analysis codes



or directly in **VS Code**, which works perfectly.

---

## ğŸ“Œ 7. Do I need Anaconda?
**No.**  
Anaconda is optional.  
This project works with a normal Python installation using:

pip install -r requirements.txt



---

## ğŸ“Œ 8. What dependencies are required?
Key packages include:

- pandas  
- matplotlib  
- scikit-learn  
- pytz  
- chardet  

All versions are listed in `requirements.txt`.

SQLite does not require installation (comes with Python).

---

## ğŸ“Œ 9. Why is the cross-source validation RÂ² negative?
Because:

- Waypoint speeds = instantaneous GPS speed  
- Trajectory speeds = segment-averaged speed  
- Optional waypoint map-matching was **not performed**

Therefore, the variability between GPS readings and link-level speeds is expected.  
This is **not an error** but a measurement-difference effect.

---

## ğŸ“Œ 10. Why does waypoint not have a â€œbeforeâ€ version like trajs?
The FHWA tool does **not** generate `waypoint_synthesized`.  
Waypoint â€œbeforeâ€ is simply the raw `waypoints.csv` inside **Synthesize Data.zip**.

---

## ğŸ“Œ 11. What files should NOT be uploaded to GitHub?
Do **not** upload any of the following:

- `unified_database.db`  
- Any large CSV files  
- Raw FHWA datasets  
- Outputs generated by the tool  

These are ignored using `.gitignore`.

---

## ğŸ“Œ 12. How do I reproduce the project from scratch?
1. Download FHWA DCFT tool  
2. Extract **Synthesize Data.zip**  
3. Set tool **INPUT** to raw data  
4. Set tool **OUTPUT** to your projectâ€™s `Output/` folder  
5. Run the tool  
6. Run Python scripts:

python src/basic_data_cleaner.py
python src/time_standardization_processor

7. Open `analysis.ipynb` and run all cells  

---

## ğŸ“Œ 13. Does the repository include all required data?
No â€” only sample files and documentation.  
Large datasets must be generated locally using the FHWA tool.

---

## ğŸ“Œ 14. Where are references and citations?
They are included in the README under **References** and **Acknowledgements**, following FHWA's citation guidelines.

---

## ğŸ“Œ 15. Can I use this pipeline on other datasets?
Yes.  
The FHWA tool and Python scripts are designed to work with similar connected-vehicle or GPS-based datasets, as long as they follow the same schema.

---

If you have additional questions, open an Issue in the repository or contact the project author.

